---
title: "G0O02a: Statistical Data Analysis: Project 1"
author: "Ege Ozan Özyedek"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, 
                      results = "hide", fig.width = 10, fig.height = 6, 
                      fig.align = 'center')
```

```{r imports}
library(ggplot2) # general plotting
library(GGally) # ggplot pairs
library(gridExtra) # ggplot subplots
library(cowplot) # to plot multiple subplots in the same pane
library(psych) # cool combinations, used to view two variable interactions in detail
library(car) # another cool plotter
library(robustbase) # used for MCD
library(rrcov) # PCA - Robust & Classic
library(wordspace) # RowNorm, used to find RMSE and ME
library(sp) #point.in.polygon
library(MASS) # Mahalanobis distance
library(factoextra) # PCA plots, analysis etc.
library(ggrepel) # For overlapping texts on ggplot
library(moments) # Skewness function
```

# Loading Data

```{r}
set.seed(0922060) # use your student number instead of 0012345
rice_dataset <- read.csv("Rice_Dataset.csv")
mygroup <- which(rmultinom(1, 1, rep(1/4, 4)) == 1)
mysample <- sample(which(rice_dataset$CLASS == mygroup), 400)
mydata <- data.frame(rice_dataset[mysample, 1:10])
```

# Questions

## Question 1

```{r Q1 - Color Definitions}

# Class is 4, Karacadag
# I'm also declaring colors for continuum

# Taken from: https://stackoverflow.com/questions/9563711/r-color-palettes-for-many-data-classes
c16 = c("dodgerblue2", "#E31A1C", "green4", "#6A3D9A", "#FF7F00", "black", "gold1", "skyblue2", "palegreen2", "#FDBF6F", "gray70", "maroon", "orchid1", "darkturquoise", "darkorange4", "brown")

pri_color <- c16[1]
sec_color <- c16[2]

```

```{r Q1 - Overview, fig.height = 12, fig.width = 12}

# First doing an overall view of all columns/variables in the dataframe

lower_plots <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
  geom_point(shape=16, colour="black", fill="grey")+
  geom_smooth(method=lm, colour=pri_color, size=0.5)
}


diag_plots <- function(data, mapping, ...) {
    ggplot(data = data, mapping = mapping) +
    geom_histogram(aes(y=..density..), colour="black", fill="grey", ...)+
    geom_density(alpha=.2, fill=pri_color) 
} 


upper_plots <- function(data, mapping, ...){
  x_val <- as.numeric(data[,as_label(mapping$x)])
  y_val <- as.numeric(data[,as_label(mapping$y)])
  cor_value <- cor(x_val, y_val, use = "complete.obs")
  font_size <- (abs(cor_value) + 0.2) * 7
  ggplot() +
    geom_text(aes(x=0.5, y=0.5, label = sprintf("%.2f", cor_value)), size = font_size) +
    theme_void()
}


p <- ggpairs(mydata, 
             lower = list(continuous = lower_plots), 
             diag = list(continuous = diag_plots),
             upper = list(continuous = upper_plots)) + 
  theme(axis.text.x = element_text(size = 7), axis.text.y = element_text(size = 7))

# It does take wee bit to plot
print(p)


# Below is the psych version of what this cell does. This is great too, but I wanted to mess around with ggplot and learn more. Alsom ggpairs gives more control over the plots themselves.

# psych::pairs.panels(mydata, smooth=TRUE, density=TRUE, ellipses=TRUE,
#                     scale= TRUE,
#                     method="pearson", pch=20, lm=TRUE, cor=TRUE,
#                     hist.col=pri_color, cex.cor=1, alpha=.01, scatter.col=sec_color)

```

```{r Q1 - Details, fig.height = 4, fig.width = 6}

# Some in-depth looks

# Quadratic relationship between Ecc and K
psych::scatter.hist(mydata$ECCENTRICITY, mydata$ASPECT_RATIO, pch=19,
             ellipse=FALSE, smooth=TRUE, xlab="Eccentricity", ylab="Aspect Ratio")

# Positive linear relationship between A and C
psych::scatter.hist(mydata$AREA, mydata$CONVEX_AREA, pch=19,
             ellipse=FALSE, smooth=TRUE, xlab="Area", ylab="Convex Area")

# Negative linear relationship between R and K
# Also, some outliers can be guessed when looking at the ellipses
psych::scatter.hist(y=mydata$ROUNDNESS, x=mydata$ASPECT_RATIO, pch=19,
             ellipse=TRUE, smooth=TRUE, ylab="Roundness", xlab="Aspect Ratio")

```

### Discussion:

-   **Data**

    -   My random portion of the dataset only contains samples/observations from a single class. That class is Karacadag (class 4).

    -   All variables are qualitative and continuous.

-   **Observations**

    -   It can be observed that most variables have high correlation with each other. These can be observed in the pairs plot that contains scatter plots, histograms and the correlation values between each variable.

    -   Most notably are the following (perfectly) linearly positively correlated variables: {(Area, Equivalent Diameter), (Area, Convex Area), (Convex Area, Equivalent Diameter)}.

    -   There are also notable negatively linear correlations, such as {(Aspect Ratio, Roundness), (Minor Axis, Eccentricity)}.

    -   The duo (Aspect Ratio, Eccentricity) displays a polynomial (in this case quadratic, convex) relationship.

    -   Some variables seem to follow a normal-like distribution (indicators here are symmetry and zero-skew, single peak at mean/middle, e.g. histogram of Major Axis).

    -   Aspect Ratio seems to be right-skewed, while Solidity is strongly skewed to the left. Eccentricity also is slightly left-skewed.

    -   Generally, no outliers are outright visible, but two points come up when looking at Roundness' scatter plots. These two points are always outside of the imaginary cluster of data. Of course, nothing definitive can be said at this point.

## Question 2

```{r Q2 - Check Normality}


check_normality <- function(x, col_name) {
  
  hplot <- ggplot(mydata, aes(x=x)) + 
    geom_histogram(aes(y=..density..), colour="black", fill="grey")+
    geom_density(alpha=.2, fill=pri_color) +
    labs(x=col_name, y="Density")
  
  qqplot <- ggplot(mydata, aes(sample=x)) + 
    stat_qq() + 
    stat_qq_line(color=pri_color) +
    labs(x="Theoretical Quantile", y=col_name)
  
  gp <- plot_grid(hplot, qqplot, labels = "AUTO", nrow=2)
  title <- ggdraw() + 
  draw_label(sprintf("%s: Plots for Normality Check", col_name), fontface = 'bold', x = 0, hjust = 0) +
  theme(plot.margin = margin(0, 0, 0, 7))
  gp <- plot_grid( title, gp, ncol = 1, rel_heights = c(0.1, 1))
  print(gp)
  
  sw <- shapiro.test(x)
  p <- sw$p.value
  
  if (p < 0.05)
    sprintf("%s: Normality cannot be assumed with 0.05 > p=%.5f", col_name, p) 
  else 
    sprintf("%s: Normality can be assumed with 0.05 < p=%.5f", col_name, p)
  
    # outliers <- Boxplot(x)
    # print(outliers)
  
}

check_normality(mydata$ECCENTRICITY, "Eccentricity")
check_normality(mydata$ASPECT_RATIO, "Aspect Ratio")
check_normality(mydata$SOLIDITY, "Solidity")


# I've also tried to run this function with mapply, works the same. I think the
# above one looks cleaner since this question in the assignment only asks to consider 3 columns. 
# If more variables were to br considered then mapply would look cleaner probably.

# arg_cols <- c("ECCENTRICITY", "SOLIDITY", "ASPECT_RATIO")
# m <- mapply(check_normality, mydata[, arg_cols], arg_cols, SIMPLIFY=FALSE, USE.NAMES=TRUE)
# print(m)


```

```{r Q2 - Transform}



# Eccentricity
Ec <- mydata$ECCENTRICITY

bcEc <- boxCox(Ec~1, lambda=seq(1,6,0.01))
lbcBc <- bcEc$x[which.max(bcEc$y)]

ptEc <- powerTransform(Ec~1)


# Aspect Ratio
K <- mydata$ASPECT_RATIO

bcK <- boxCox(K~1, lambda=seq(-3,1,0.01))
lbcK <- bcK$x[which.max(bcK$y)]

ptK <- powerTransform(K~1)

tK <- bcPower(K, ptK$roundlam)


# Solidity

S <- mydata$SOLIDITY

# This is just for show, I do not apply Box-Cox in the end
bcS <- boxCox(S~1, lambda=seq(90, 140, 0.5))
lbcS <- bcS$x[which.max(bcS$y)]

ptS <- powerTransform(S~1)

# Prints
sprintf("Eccentricity: λ (boxCox) = %.5f, λ (powerTransform) = %.5f, λ rounded = %.5f", lbcBc, ptEc$lambda, ptEc$roundlam)
sprintf("Aspect Ratio: λ (boxCox) = %.5f, λ (powerTransform) = %.5f, λ rounded = %.5f", lbcK, ptK$lambda, ptK$roundlam)
sprintf("Solidity: λ (boxCox) = %.5f, λ (powerTransform) = %.5f, λ rounded = %.5f", lbcS, ptS$lambda, ptS$roundlam)

# Transform vars

tEc <- bcPower(Ec, lbcBc)

tK <- bcPower(K, ptK$roundlam)

tS <- log(S/(1-S))

skewness(S)
skewness(tS)

```

```{r Q2 - Re-asses Normality of Transformed Variables}

check_normality(tEc, "Eccentricity (Box-Cox)")
check_normality(tK, "Aspect Ratio (Box-Cox)")
check_normality(tS, "Solidity (Logit)")
```

```{r Q2 - Use Transformed Variables}

# While re-running transformations above (i.e. while the transformations are done and are in the workspace) it can create problems. If the code is run sequentially everything is alright.

mydata[, "SOLIDITY"] <- tS
mydata[, "ECCENTRICITY"] <- Ec
mydata[, "ASPECT_RATIO"] <- tK
```

### Discussion:

For this question, I've implemented the `checkNormality` function, which plots the histogram and Quantile-Quantile plot of a variable, and tests the distribution of the variable for normality using the Shapiro-Wilk test. The normality of a variable is rejected for p-values below 0.05. Now we analyse each variable and transform if need be.

-   **Eccentricity:**: As mentioned before, the histogram of Eccentricity, although normal-like, shows slight left-skewness. The left-skew is also apparent in the Q-Q plot (deviation from the line at the bottom left). The Shapiro test rejects the normality of the variable, not a surprise following the aforementioned observations. A power transformation is in order. Box-Cox is applied, first on the [-2, 2] range. The log-likelihood plot increases towards positive values of lambda, hence we change our scale to [1, 6]. The appropriate lambda value is found to be around 3.68. The `powerTransform` function is also applied to obtain a rounded lambda. It finds a slightly different lambda value, and gives a rounded lambda of 2 (this value can be double checked in the log-likelihood plot, it is in the 95% CI). For the transformation of Eccentricity, λ = 3.68 is used. After the transformation we again observe the histogram and Q-Q plot, as well as the Shapiro-Wilk test. The Shapiro test does not reject the transformed Eccentricity, hence we can assume normality. Additionally, a reduction of the left-skew can be seen in the histogram and Q-Q plot.

-   **Aspect Ratio** Similarly, a right-skew was observed on Aspect Ratio. This is again apparent, both in the histogram and Q-Q plot, a bit more severe compared to Eccentricity's skewness. Some possibility of outliers can also be observed in the histogram, which may be causing the skew. The Shapiro-Wilk test rejects the normality of Aspect Ratio, again a Box-Cox transformation is in order to transform the variable to have a normal-like distribution. Lambda values in the [-2, 2] range are considered. We again readjust the range, to [-3.5, 1.5], and find the lambda value to be around -0.98. `powerTransform` outputs a slightly lower lambda, and a rounded value of 0 (which turns the Box-Cox transformation to ln(K)). For the transformation of Aspect Ratio, λ = 0 is used. Although it does not necessarily have an advantage in this case, using rounded lambda values can simplify the transformation which would increase the explainability of statistical values and transformations. Since it is still in the 95% confidence interval, it is still viable, the transformation just won't be as drastic. This can be seen in the normality check of the transformed aspect ratio. Although the skew is reduced, it still exists slightly. The Shapiro test does not reject the transformed Aspect Ratio, hence we can assume normality.

-   **Solidity**: Solidity has a heavy left-skew, as it can be seen through the histogram and Q-Q. This might be a cause of outliers; however, at this point we cannot rule out any point as anomalous. First, I applied Box-Cox, and the resulting lambda value came to around 113. This is a very large value, which leads me to believe that Solidity is not normally distributed and follows another distribution. For this reason, the goal of the transformation changes to reducing the skewness, rather than making the variable normal-like. To achieve this, the logit function is applied to Solidity (which has values in the [0, 1] interval), which does reduce the skewness (from -1.6 to -0.9).

## Question 3

```{r Q3 - Prepare Ellipses}

perimiter.minor_axis <- data.frame(x = mydata$PERIMETER, y = mydata$MINOR_AXIS)
rownames(perimiter.minor_axis) <- rownames(mydata)
perimiter.minor_axis.mean <- colMeans(perimiter.minor_axis)
perimiter.minor_axis.cov <- cov(perimiter.minor_axis)
cutoff<- sqrt(qchisq(0.99, 2))

mcd50 <- covMcd(perimiter.minor_axis, alpha = 0.5)
mcd25 <- covMcd(perimiter.minor_axis, alpha = 0.25)


tolerance.classic <- car::ellipse(perimiter.minor_axis.mean, perimiter.minor_axis.cov, cutoff, draw=FALSE)
tolerance.mcd50 <- car::ellipse(mcd50$center, mcd50$cov, cutoff, draw=FALSE)
tolerance.mcd25 <- car::ellipse(mcd25$center, mcd25$cov, cutoff, draw=FALSE)

ellipse.classic <- as.data.frame(tolerance.classic)
ellipse.mcd50 <- as.data.frame(tolerance.mcd50)
ellipse.mcd25 <- as.data.frame(tolerance.mcd25)

# Below, I'm taking the points that are not in the intersection of ellipses to label and color them.
findSuspects <- function(pts, ell) {as.logical(point.in.polygon(pts$x, pts$y, ell$x, ell$y))}
suspects <- findSuspects(perimiter.minor_axis, ellipse.classic) & findSuspects(perimiter.minor_axis, ellipse.mcd25) & findSuspects(perimiter.minor_axis, ellipse.mcd50)
perimiter.minor_axis.detailed <- data.frame(perimiter.minor_axis, suspect = suspects)
rownames(perimiter.minor_axis.detailed) <- rownames(mydata)



```

```{r Q3 - Plot Ellipses}
# Plotting the ellipse along with the data points
ggplot(perimiter.minor_axis.detailed, aes(x, y)) +
  geom_path(data=ellipse.classic, aes(x, y, linetype="Classic"),  color = pri_color, size=1) + geom_point(aes(x=perimiter.minor_axis.mean[1], y=perimiter.minor_axis.mean[2]), col=pri_color, size=2, shape=10) + 
  geom_path(data=ellipse.mcd25, aes(x, y, linetype="MCD25"), color = c16[3], size=1) + geom_point(aes(x=mcd25$center[1], y=mcd25$center[2]), color=c16[3], size=2, shape=10) +
  geom_path(data=ellipse.mcd50, aes(x, y, linetype="MCD50"), color = sec_color, size=1) + geom_point(aes(x=mcd50$center[1], y=mcd50$center[2]), color=sec_color, size=2, shape=10) +
  # extra stuff
  geom_point(aes(col=suspect)) + 
  scale_linetype_manual(name="Ellipses", values=c("Classic"="solid", "MCD50"="solid", "MCD25"="solid"), guide=guide_legend(override.aes = list(colour=c("Classic"=pri_color, "MCD25"=c16[3], "MCD50"=sec_color)))) +
  scale_color_manual(name = "Points", values = c("TRUE" = "black", "FALSE" = c16[5]), labels=c("TRUE"="Data", "FALSE"="Possible Outliers")) + 
  geom_text(data = subset(perimiter.minor_axis.detailed, suspect == FALSE), aes(label = rownames(subset(perimiter.minor_axis.detailed, suspect == FALSE))), size=2.5) +
  labs(title="Ellipses over the Perimeter vs Minor Axis", x="Perimeter", y="Minor Axis") 
  


# Below is the non-ggplot version of plotting the ellipses and data

# plot(X, pch=19)
# car::ellipse(m, cov(perimiter.minor_axis), cutoff)
# car::ellipse(mcd50$center, mcd50$cov, cutoff)
# car::ellipse(mcd25$center, mcd25$cov, cutoff)

```

```{r Q3 - Plot Distances}

# I couldn't change the shape of each point in the scatter plot that displays the distances 
# (part of the package that has MCD calculations), so I'm also doing a custom plot with ggplot that shows the distances. 
# Hopefully its good practice for the exam and the next project.

customDistancePlot <- function(mahdists, labels, name) {
  
  cutoff<-sqrt(qchisq(0.99, 2))
  dists <- data.frame(x = c(1:length(mahdists)), y = mahdists, labels=labels)
  dists$outlier = (sqrt(dists$y) > cutoff)
  
  if (name == "Classic")
    yaxislab = "Square Root of Mahalanobis Distances"
  else
    yaxislab = "Square Root of Robust Distances" # Robust distances are still mahalanobis distances, but computed with the cov matrix and mean vec of the %50/%25 subset

  ggplot(data = dists , aes(x = x, y = sqrt(y))) +
  geom_point(aes(col=dists$outlier)) + 
  scale_color_manual(values=c("TRUE"=c16[5], "FALSE"="black")) + 
  geom_text(data = subset(dists, outlier == TRUE), aes(label = subset(dists, outlier == TRUE)$labels), size=2.5) +
  geom_hline(aes(yintercept = cutoff), color=pri_color,linetype="dashed") + 
  labs(title = sprintf("%s: Distance Plot", name), x = "Index", y = yaxislab, color = "Robust Distance") + 
  theme(legend.position="none") +
  ylim(0, 5.5)
  
}


classicdists <- mahalanobis(perimiter.minor_axis, perimiter.minor_axis.mean, perimiter.minor_axis.cov)

gp1 <- customDistancePlot(classicdists, rownames(mydata), "Classic")
gp2 <- customDistancePlot(mcd50$mah, rownames(mydata), "MCD50")
gp3 <- customDistancePlot(mcd25$mah, rownames(mydata), "MCD25")


gp <- plot_grid(gp1, gp2, gp3, labels = "AUTO", ncol=3)
title <- ggdraw() + 
  draw_label("Distance Plots", fontface = 'bold', x = 0, hjust = 0) + 
  theme(plot.margin = margin(0, 0, 0, 7))
gp <- plot_grid( title, gp, ncol = 1, rel_heights = c(0.1, 1)) 
print(gp)

# Below is the non-ggplot version
# plot(mcd50, which="distance", cutoff=sqrt(qchisq(0.99, 2)), classic=TRUE, labels.id = rownames(mydata), cex.id=0.6, label.pos = c(1,2), pch=10)
# plot(mcd25, which="distance", cutoff=sqrt(qchisq(0.99,2)), classic=TRUE)






```

### Discussion:

The first plot in this question showcases the threshold ellipses with the classic mean and variance (of the entire dataset), and the MCD-based ellipses with %50 and %25 breakdown values. As expected, the ellipse calculated with the classical mean and covariance is the least restrictive. Since it includes the possible outliers in the calculation of the mean and covariance, it is not robust. However, some points still fall out of the threshold. MCD50 is more restrictive compared to the classic threshold ellipse, since it first finds an outlier free subset of the data and calculates the mean (vector) and covariance (matrix) based on these values (and then calculates the Mahalanobis distance based on these two elements). MCD50 flags slightly more points as outliers. MCD25 is even more restrictive since it finds a subset that only contains %25 of the data (which minimizes the determinant of its covariance matrix) and computes the mean and covariance matrix with those. It can be seen that this is indeed true in the plot, as it flags many points in MCD50 and Classic as outliers.

Generally, the expected amount of outliers are far far less than %75 (for MCD25). Hence, MCD25 seems to be overkill for this problem, unless there is some domain information that indicates the data at hand has been subject to faulty measurements. Since it uses a smaller subset of data, it does not accurately capture the covariance matrix of the theorized data distribution. MCD50 and the classic threshold ellipses are very similar, and MCD50 does have the added robustness since it considers a subset without possible outliers. It's then beneficial to use the MCD method, but the restriction on the data subset/breakdown value (75, 50, 25) depends on the expected amount of outliers that are in the dataset.

Following the previous discussion, the distance plots also showcase how data gets more dispersed (far away from each other, w.r.t. Mahalanobis distance) as the subset used changes. A smaller subset captures less variation in data, hence more points are farther away from the mean for MCD25, compared to the others. In the same breath, MCD50 seems to capture the variation of the whole dataset accurately, since its distance plot similar to the distance plot of the classical threshold ellipse.

## Question 4

```{r Q4 - Data Prep}
mytrainingdata <- mydata[1:200, ]
myvalidationdata <- mydata[201:400, ]
```

```{r Q4 - Robust PCA}

# I wanted to emphasize the cumulative variance, so I've plotted it (i.e. screeplot) with ggplot. There is also the function screeplot which plots bars that showcase the variance encapsulated per PC. 
# With help from: https://stackoverflow.com/questions/75799195/create-scree-plot-with-primary-and-secondary-y-axis-in-r

customScreePlot <- function(var) {
  ggplot(mapping=aes(x=c(1:10), y=cumsum(var)) ) + 
  geom_point(size = 1) +
  geom_line() + 
  labs(title="Scree Plot", x="# Principle Components", y = "Cumulative Variance") +
  geom_hline(yintercept=0.99, linetype='dotted', col = pri_color) +
  annotate("text", x = 7, y = 0.95, label = expression(sigma[k]^2 > 0.99 * sigma[N]^2) , vjust = -0.5) +
  geom_hline(yintercept=0.80, linetype='dotted', col = sec_color) +
  annotate("text", x = 7, y = 0.76, label = expression(sigma[k]^2 > 0.8 * sigma[N]^2), vjust = -0.5) +
  scale_x_continuous(breaks=c(1:10))
}

training.pca <- prcomp(mytrainingdata, scale=TRUE)
summary(training.pca)

var = training.pca$sdev^2 / sum(training.pca$sdev^2)

customScreePlot(var)

# We can either choose k=2 (>%80 of variance), or the elbow point (k = 3). 

chosen_k = 3

training.robpca <- PcaHubert(mytrainingdata, k=chosen_k, scale=mad, crit.pca.distances=0.99)

summary(training.robpca)
plot(training.robpca, pch=19, id.n.od=4)

training.robpca.outliers <- which(training.robpca$od > training.robpca$cutoff.od)# training.robpca$sd > training.robpca$cutoff.sd
mytrainingdata.clean <- mytrainingdata[-training.robpca.outliers, ]

```

### Discussion:

For both PCA methods used, data is scaled (this is not explicitly done, but passed as a function argument). The data at hand contains variables that do not share the same interval, and are not of the same measure. Hence, it is important to scale the data to accurately capture the variance of observations. For PcaHubert, robust scaling is done (the mad function is passed as the argument) as recommended in the lecture notes.

The choice of the amount of principle components depends on the cumulative variance captured for k components. For this purpose, the Scree Plot is plotted. Here, we can choose either the knee/elbow point, or the k value that lies on a variance threshold (i.e. 80/90/99%). In the scree plot it can be observed that k=2 or k=3 are good choices. k=2 captures enough characteristics of the data (\~86%), and would enable easier interpretation since it's two dimensional. k=3 captures nearly all variance in the data (\~99%), and still easy to interpret. I've chosen k=3 since it maximizes the cumulative variance. However, k=2 can also be preferred if a lower dimension is desired.

The two bad leverage points (i.e. outliers, indexed 19737 and 17482) were also outside of all three ellipses in the previous question. The other two orthogonal outliers (indexed 17790, 15477) were only flagged by MCD25. The orthogonal outliers and bad leverage points are omitted for the rest of the assignment.

## Question 5

```{r Q5 - Classic PCA (wo outliers)}

training.pca <- prcomp(mytrainingdata.clean, scale=TRUE)
summary(training.pca)

var = training.pca$sdev^2 / sum(training.pca$sdev^2)

customScreePlot(var)


training.pca.clean <- PcaClassic(mytrainingdata.clean, k=chosen_k, crit.pca.distances=0.99, scale=TRUE)
getPrcomp(training.pca.clean)

plot(training.pca.clean, pch=19)



```

### Discussion:

Again, I'm choosing k=3. The motivating factors for this choice are

-   It's the elbow point.

-   3 components capture \>99% of the variance in the original dataset, which is desired.

## Question 6

```{r Q6 - Plot Biplots, fig.width=12, fig.height=6}

# Again, for analysis biplot probably is enough (there is also fviz_biplot as well), but 
# I decided to do the ggplot version to color the arrows and suppress the point labels, which convolute the biplot in my opinion. 
customBiplot <- function(arg.pca, detailed=TRUE) {
  scores <- as.data.frame(training.pca.clean$scores)
  loadings <- as.data.frame(training.pca.clean$loadings)
  
  gp1 <- ggplot() +
  geom_point(data = scores, aes(x = PC1, y = PC2), size = 1) +
  geom_segment(data = loadings, aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5, col=rownames(loadings)), arrow = arrow(length = unit(0.1, "inches")), size=0.8) +
  labs(title = "PCA Biplot (PC1 & PC2)", x = "Principal Component 1", y = "Principal Component 2") +
  theme(legend.title = element_blank(), legend.text = element_text(size = 8)) +
  scale_color_manual(values=c16) +
   geom_text_repel(data = loadings, aes(x = PC1 * 5, y = PC2 * 5, label = rownames(loadings)), size=3)
  
  if (dim(loadings)[2] > 2) {
    
    gp2 <- ggplot() +
      geom_point(data = scores, aes(x = PC2, y = PC3), size = 1) +
      geom_segment(data = loadings, aes(x = 0, y = 0, xend = PC2 * 5, yend = PC3 * 5, col=rownames(loadings)), arrow = arrow(length = unit(0.1, "inches")), size=0.8) +
      labs(title = "PCA Biplot (PC2 & PC3)", x = "Principal Component 2", y = "Principal Component 3") +
      theme(legend.title = element_blank(), legend.text = element_text(size = 8)) +
      scale_color_manual(values=c16) +
      geom_text_repel(data = as.data.frame(training.pca.clean$loadings), aes(x = PC2 * 5, y = PC3* 5, label = rownames(loadings)), size=3)

    
    gp3 <- ggplot() +
      geom_point(data = scores, aes(x = PC3, y = PC1), size = 1) +
      geom_segment(data = loadings, aes(x = 0, y = 0, xend = PC3 * 5, yend = PC1 * 5, col=rownames(loadings)), arrow = arrow(length = unit(0.1, "inches")), size=0.8) +
      labs(title = "PCA Biplot (PC3 & PC1)", x = "Principal Component 3", y = "Principal Component 1") +
      theme(legend.title = element_blank(), legend.text = element_text(size = 8)) +
      scale_color_manual(values=c16) +
      geom_text_repel(data = loadings, aes(x = PC3 * 5, y = PC1 * 5, label = rownames(loadings)), size=3)
    
    gp <- plot_grid(gp1 + theme(legend.position="none"), gp2 + theme(legend.position="none"), gp3 + theme(legend.position="none"), labels = "AUTO", ncol=3)
    
    gp <- plot_grid(title, gp, ncol = 1, rel_heights = c(0.1, 1)) 
    title <- ggdraw() + 
    draw_label("Plots for Normality Check", fontface = 'bold', x = 0, hjust = 0) +
    theme(plot.margin = margin(0, 0, 0, 7))
    
    legend <- get_legend(gp1 + guides(color = guide_legend(nrow = 1)) +
    theme(legend.position = "bottom"))
    gp <- plot_grid(gp, legend, ncol = 1, rel_heights = c(1, .1))
    
    
    print(gp)
    
    if (detailed) {
      print(gp1 + geom_text_repel(data = scores, aes(x = PC1, y = PC2, label = rownames(scores)), size=2, color="maroon"))
      print(gp2 + geom_text_repel(data = scores, aes(x = PC2, y = PC3, label = rownames(scores)), size=2, color="maroon"))
      print(gp3 + geom_text_repel(data = scores, aes(x = PC3, y = PC1, label = rownames(scores)), size=2, color="maroon"))

    }
    
  } 
  
  else 
    gp1
}


plots <- customBiplot(training.pca.clean)


# # Below is the base biplot version of what is plotted above
# biplot(training.pca.clean)
# biplot(x=training.pca.clean)

# # Below is the version using fviz_pca_biplot, these look better than the base biplots 
# training.prcomp.clean <- getPrcomp(training.pca.clean)
# fviz_pca_biplot(training.prcomp.clean, axes=c(1, 2), col.var="red", repel=TRUE)
# fviz_pca_biplot(training.prcomp.clean, axes=c(2, 3), col.var="red", repel=TRUE)
# fviz_pca_biplot(training.prcomp.clean, axes=c(3, 1), col.var="red", repel=TRUE)

```

### Discussion:

The biplots for each component combination can be seen in the plots above.

There are three things to consider while analyzing these biplots:

-   **Direction of the Arrows**: The direction of an arrow showcases the direction in which variance for that variable is maximum, and also how much it contributes to a PC. For example, in Biplot (PC1 vs PC2), Convex Area (or Area, or Equivalent Diameter) affects the 1st component strongly (i.e. changes in value for that variable are captured in the component), while its effect on PC2 is minimal (change in this variable does not effect an observation's representation in the 2nd component greatly). Also to explain the first point a bit further, moving in the direction of the Convex Area arrow, the reduced dimension representation of observations (i.e. scores) have values that vary in Convex Area, but if we were to move orthogonal to the arrow (almost on the y-axis) all observations here would have very similar Convex Area values (a.k.a almost no variance is captured).

-   **Length of Arrows**: If an arrow is longer, it's influence on the PC(s) is greater, and if it's shorter it does not influence the PC(s) a lot. For example, in Biplot (PC2 & PC3), Solidity is long, so going along the Solidity arrow will give observations that drastically differ in their Solidity values, and the variance is mostly captured in the 3rd PC (solidity is less potent in the other two components). As opposed to Convex Area, which has a very small influence over these two PCs (2 & 3).

-   **Correlations**: The arrows also showcase correlations on each plane. For example, in each biplot we can observe the fact that Eccentricity and Aspect Ratio are positively correlated (which was also an observation made in the first question of the assignment). We can also observe that Minor Axis and Aspect Ratio generally point in opposite directions, i.e. there exist a negative correlation between the two variables.

-   **Extra Observations**: In the first Biplot (PC1 & PC2), all variables point to somewhat the same direction (or rather on one side of the plane?). With the inclusion of the 3rd PC, more information is given on how each variable is correlated with each other. Here, with examples such as Solidity, which contributes highly to PC3, we can see the effect of the additional variance obtained by using 3 components (the variance difference between k=2 and k=3 was something around \~15%). Basically, additional information on how observations change in value is obtained by each selected principle component.

## Question 7

```{r Q7 - Calculate Errors}

calculateErrors <- function(actual, arg.pca, val=FALSE, data.name="") {
  loadings <- arg.pca$loadings
  center <- arg.pca$center
  scale <- arg.pca$scale
  
  actual.scaled <- scale(actual, center=center, scale=scale)
  
  if (val)
    scores <- actual.scaled %*% loadings
  else
    scores <- arg.pca$scores
  
  predicted <- scores %*% t(loadings)
  
  normdiff <- rowNorms(predicted - actual.scaled, method = "euclidean", p = 2)
  errors.rmse <- sqrt( mean(normdiff^2) )
  errors.me <- median(normdiff)
  
  outstring <- sprintf("%s Reconstruction: RMSE = %.2f, ME = %.2f \n", data.name, errors.rmse, errors.me)
  cat(outstring)
}

cat("--- k=3 ---\n")

calculateErrors(mytrainingdata.clean, training.pca.clean, val=FALSE, data.name = "Training Data (k=3)")
calculateErrors(myvalidationdata, training.pca.clean, val=TRUE, data.name = "Validation Data (k=3)")

# For comparison's sake

cat("\n\n--- k=2 ---\n")

training.pca.clean.twocomponents <- PcaClassic(mytrainingdata.clean, k=2, crit.pca.distances=0.99, scale=TRUE)

calculateErrors(mytrainingdata.clean, training.pca.clean.twocomponents, val=FALSE, data.name = "Training Data (k=2)")
calculateErrors(myvalidationdata, training.pca.clean.twocomponents, val=TRUE, data.name = "Validation Data (k=2)")

cat("\n\n--- k=1 ---\n")

training.pca.clean.onecomponent <- PcaClassic(mytrainingdata.clean, k=1, crit.pca.distances=0.99, scale=TRUE)

calculateErrors(mytrainingdata.clean, training.pca.clean.onecomponent, val=FALSE, data.name = "Training Data (k=1)")
calculateErrors(myvalidationdata, training.pca.clean.onecomponent, val=TRUE, data.name = "Validation Data (k=1)")



```

### Discussion:

The RMSE gives us an estimation of how accurate the PCA reconstruction is, while the median error helps us see what the difference between actual vs reconstructed observation is. For the sake of comparison, I've printed the RMSE and ME for k = {3, 2, 1}, but the discussion will first focus on the chosen number of components, k=3.

The RMSE value for the training data are low, which indicates that PCA has retained most of the characteristics of the given training samples. The validation error values are similar, albeit, slightly worse, but that is expected since these are unseen observations, and PCA is not "trained" on the variance of the validation data. However, still getting a low score on unseen (validation) data means that PCA is able to capture and accurately represent whatever distribution the original data is obtained from, and can represent the data using 3 components.

Now, if we were to compare the reconstruction errors obtained from the 3D vs 2D vs 1D PCA representations, the results are easy to guess. As the captured variance of the data decreases (as less PCs are used), the reconstruction error increases. In addition to this, the difference between the RMSE of training and validation also increases, meaning that PCA's ability to generalize also decreases as less variance is captured (or less PCs are used).
